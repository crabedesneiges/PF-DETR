# Configuration for DataModule, Model, and Training in DETR pipeline

dataset:
  path_to_train_valid: '/data/multiai/data3/HyperGraph-2212.01328/singleQuarkJet_train.root'
  num_events_train_valid: 512
  frac_train: 0.9
  path_to_test: '/data/multiai/data3/HyperGraph-2212.01328/singleQuarkJet_test.root'
  num_events_test: -1
  enable_dataloader_cache: false
  build_topocluster: true
  max_particles: 35 #15 + num_queries_notrack
  path_to_normalization_params: "data/normalization/params.json"
  num_dataloader_workers: 16
  batchsize: 256
  is_private_sample: false

model:
  aux_loss: true
  indence_matrix_prediction: true
  track_and_charged_cardinality: true
  use_DETR_V3: true
  use_auxiliary_loss: false
  use_multimodal_fusion: true
  use_cross_modal_attention: true
  no_object_weight: 0.3
  num_classes: 5
  hidden_dim: 64
  num_queries_notrack: 20
  cluster_input_dim: 8
  topo_input_dim: 8
  track_input_dim: 18
  nheads: 2
  num_encoder_layers: 1
  num_decoder_layers: 1
  charged_class_weights: [0.3, 1.0,1.0]
  neutral_class_weights: [1.0, 0.3, 1.0]
  transformer_dropout: 0.0
  mlp_dropout: 0.2

training:
  aux_loss_coeff: 0.5
  indence_matrix_loss_coeff: 0.1
  cls_loss_coeff: 1.0
  bbox_loss_coeff: 0.5
  num_epochs: 25
  learningrate: 5.0e-6
  weight_decay: 0.0001

  # Cosine Annealing LR Scheduler
  lr_scheduler:
    name: CosineAnnealingLR
    warmup_steps: 5
    warmup_start_lr: 5.0e-7
    base_lr: 5.0e-6
    T_max: 25
    eta_min: 5.0e-7
